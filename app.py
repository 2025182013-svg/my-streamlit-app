import streamlit as st
import requests, html, json, os
from datetime import datetime
from openai import OpenAI
import pandas as pd

# =====================
# ê¸°ë³¸ ì„¤ì •
# =====================
st.set_page_config(page_title="RefNote AI", layout="wide")
st.title("ğŸ“š RefNote AI")
st.caption("ì—°êµ¬/ë‰´ìŠ¤ ë¶„ë¦¬í˜• ë¦¬ì„œì¹˜ ìƒì„± ì‹œìŠ¤í…œ Â· APA7 Â· íˆìŠ¤í† ë¦¬ ë³µì› Â· CSV ë‹¤ìš´ë¡œë“œ")

HISTORY_FILE = "history.json"

# =====================
# ì„¸ì…˜ ìƒíƒœ
# =====================
if "results" not in st.session_state:
    st.session_state.results = None
if "history" not in st.session_state:
    st.session_state.history = []

# =====================
# ì‚¬ì´ë“œë°” API
# =====================
st.sidebar.header("ğŸ”‘ API ì„¤ì •")
openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
naver_id = st.sidebar.text_input("Naver Client ID", type="password")
naver_secret = st.sidebar.text_input("Naver Client Secret", type="password")
newsapi_key = st.sidebar.text_input("NewsAPI Key (ê¸€ë¡œë²Œ ë‰´ìŠ¤, ì„ íƒ)", type="password")

if not openai_key:
    st.warning("â¬…ï¸ OpenAI API Key í•„ìˆ˜")
    st.stop()

client = OpenAI(api_key=openai_key)

# =====================
# ìœ í‹¸
# =====================
def clean(t):
    return html.unescape(t).replace("<b>", "").replace("</b>", "").strip()


def parse_date(d):
    try:
        return datetime.strptime(d, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None


def format_source(domain):
    return domain.replace("www.", "").split(".")[0].capitalize()


def apa_news(row):
    author = row.get("ì €ì", row.get("ì¶œì²˜", "Unknown"))
    year = row.get("ë°œí–‰ì¼", "")[:4] if row.get("ë°œí–‰ì¼") else "n.d."
    return f"{author}. ({year}). {row['ì œëª©']}. {row['ì¶œì²˜']}. {row['ë§í¬']}"

# =====================
# AI
# =====================
def gen_questions(topic):
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ ì—°êµ¬ ì§ˆë¬¸ 3ê°œ ìƒì„±:\n{topic}"}],
        temperature=0.3
    )
    return [q.strip("-â€¢ ") for q in r.choices[0].message.content.split("\n") if q.strip()]


def gen_keywords(topic):
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"ë‹¤ìŒ ì£¼ì œ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¤‘ìš”ë„ìˆœ ì‰¼í‘œ ì¶œë ¥:\n{topic}"}],
        temperature=0.2
    )
    return [k.strip() for k in r.choices[0].message.content.split(",")]


def gen_trend_summary(keywords):
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì‹  ì—°êµ¬ë™í–¥ ìš”ì•½:\n{', '.join(keywords)}"}],
        temperature=0.2
    )
    return r.choices[0].message.content.strip()


def relevance(topic, n):
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"ì—°êµ¬ì£¼ì œ:{topic}\nì œëª©:{n['ì œëª©']}\nìš”ì•½:{n['ìš”ì•½']}\nê´€ë ¨ë„ 0~3 ìˆ«ìë§Œ"}],
        temperature=0
    )
    try:
        return int(r.choices[0].message.content.strip())
    except:
        return 0

# =====================
# ë¶„ë¥˜ê¸°
# =====================
def classify_topic(topic):
    if any(k in topic for k in ["ë¹„êµ", "vs", "ì •ì±…", "ì œë„", "êµ­ê°€", "ëª¨ë¸"]):
        return "research"
    return "news"

# =====================
# ë‰´ìŠ¤
# =====================
def search_news_korea(q):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": naver_id, "X-Naver-Client-Secret": naver_secret}
    params = {"query": q, "display": 30, "sort": "date"}
    r = requests.get(url, headers=headers, params=params).json()
    out = []
    for i in r.get("items", []):
        out.append({
            "ì œëª©": clean(i["title"]),
            "ìš”ì•½": clean(i["description"]),
            "ì¶œì²˜": format_source(i["link"].split("/")[2]),
            "ë°œí–‰ì¼": parse_date(i["pubDate"]).strftime("%Y-%m-%d") if parse_date(i["pubDate"]) else "",
            "ë§í¬": i["link"]
        })
    return out


def search_news_global(q):
    if not newsapi_key:
        return []
    url = "https://newsapi.org/v2/everything"
    params = {"q": q, "language": "en", "sortBy": "publishedAt", "pageSize": 30, "apiKey": newsapi_key}
    r = requests.get(url, params=params).json()
    out = []
    for i in r.get("articles", []):
        out.append({
            "ì œëª©": i.get("title"),
            "ìš”ì•½": i.get("description"),
            "ì¶œì²˜": i.get("source", {}).get("name", "NewsAPI"),
            "ë°œí–‰ì¼": i.get("publishedAt", "")[:10],
            "ë§í¬": i.get("url")
        })
    return out

# =====================
# ë…¼ë¬¸ (CrossRef)
# =====================
def search_crossref(q):
    url = "https://api.crossref.org/works"
    params = {"query": q, "rows": 20}
    r = requests.get(url, params=params).json()
    out = []
    for i in r.get("message", {}).get("items", []):
        out.append({
            "ì œëª©": i.get("title", [""])[0],
            "ì €ì": ", ".join([f"{a.get('family','')} {a.get('given','')}" for a in i.get("author", [])]),
            "í•™ìˆ ì§€": i.get("container-title", [""])[0],
            "ì—°ë„": i.get("published-print", {}).get("date-parts", [[""]])[0][0] if i.get("published-print") else "",
            "ë§í¬": i.get("URL")
        })
    return pd.DataFrame(out)

# =====================
# ì…ë ¥
# =====================
topic = st.text_input("ì—°êµ¬ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if st.button("ğŸ” ë¦¬ì„œì¹˜ ì‹œì‘") and topic:
    with st.spinner("ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘..."):
        mode = classify_topic(topic)
        questions = gen_questions(topic)
        keywords = gen_keywords(topic)
        trend = gen_trend_summary(keywords)

        news_list = []
        if mode == "news":
            for k in keywords[:2]:
                news_list.extend(search_news_korea(k))
        else:
            for k in keywords[:2]:
                news_list.extend(search_news_global(k))
                news_list.extend(search_news_korea(k))

        filtered = []
        for n in news_list:
            n["score"] = relevance(topic, n)
            if n["score"] >= 2:
                filtered.append(n)

        news_df = pd.DataFrame(filtered).drop_duplicates(subset=["ë§í¬"])
        paper_df = search_crossref(topic)

        st.session_state.results = {
            "timestamp": datetime.now().isoformat(),
            "topic": topic,
            "mode": mode,
            "questions": questions,
            "keywords": keywords,
            "trend": trend,
            "news": news_df.to_dict(orient="records"),
            "papers": paper_df.to_dict(orient="records")
        }

        # íˆìŠ¤í† ë¦¬ ì €ì¥
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                st.session_state.history = json.load(f)

        st.session_state.history.append(st.session_state.results)
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(st.session_state.history, f, ensure_ascii=False, indent=2)

# =====================
# ì¶œë ¥
# =====================
if st.session_state.results:
    r = st.session_state.results

    st.subheader("ğŸ” ì—°êµ¬ ì§ˆë¬¸")
    for q in r["questions"]:
        st.markdown(f"â€¢ {q}")

    st.subheader("ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ")
    st.write(", ".join(r["keywords"]))

    st.subheader("ğŸ“ˆ ì—°êµ¬ ë™í–¥")
    st.markdown(r["trend"])

    tab_news, tab_paper = st.tabs(["ğŸ“° ë‰´ìŠ¤", "ğŸ“„ ë…¼ë¬¸"])

    with tab_news:
        df = pd.DataFrame(r["news"])
        if not df.empty:
            sort = st.radio("ì •ë ¬", ["ê´€ë ¨ë„ìˆœ", "ìµœì‹ ìˆœ"], horizontal=True)
            if sort == "ê´€ë ¨ë„ìˆœ":
                df = df.sort_values(by="score", ascending=False)
            else:
                df = df.sort_values(by="ë°œí–‰ì¼", ascending=False)
            st.dataframe(df, use_container_width=True)
            st.download_button("ğŸ“¥ ë‰´ìŠ¤ CSV", df.to_csv(index=False).encode("utf-8-sig"), f"{r['topic']}_news.csv")

            st.subheader("ğŸ“ APA ì°¸ê³ ë¬¸í—Œ (Top10)")
            for _, row in df.head(10).iterrows():
                st.markdown(f"- {apa_news(row)}")
        else:
            st.info("ë‰´ìŠ¤ ê²°ê³¼ ì—†ìŒ")

    with tab_paper:
        pdf = pd.DataFrame(r["papers"])
        st.dataframe(pdf, use_container_width=True)
        st.download_button("ğŸ“¥ ë…¼ë¬¸ CSV", pdf.to_csv(index=False).encode("utf-8-sig"), f"{r['topic']}_papers.csv")

# =====================
# íˆìŠ¤í† ë¦¬
# =====================
st.sidebar.header("ğŸ“‚ ë¦¬ì„œì¹˜ íˆìŠ¤í† ë¦¬")
if os.path.exists(HISTORY_FILE):
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        saved = json.load(f)
else:
    saved = []

for h in reversed(saved):
    if st.sidebar.button(f"{h['topic']} ({h['timestamp'][:10]})"):
        st.session_state.results = h
        st.success("ë¦¬ì„œì¹˜ ë³µì› ì™„ë£Œ")
