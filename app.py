import streamlit as st
import requests
import pandas as pd
import re, html
from openai import OpenAI
from datetime import datetime
from urllib.parse import urlparse

# =====================
# Page Config
# =====================
st.set_page_config(page_title="RefNote AI", layout="wide")

# =====================
# Sidebar - API Keys
# =====================
st.sidebar.title("ğŸ” API Keys")
openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
naver_client_id = st.sidebar.text_input("Naver Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver Client Secret", type="password")

if not (openai_api_key and naver_client_id and naver_client_secret):
    st.sidebar.warning("API Keyë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=openai_api_key)

# =====================
# Utils
# =====================
def clean_text(text):
    text = re.sub("<.*?>", "", text)
    return html.unescape(text)

def parse_date(pub_date):
    try:
        dt = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %z")
        return dt.strftime("%Y-%m-%d")
    except:
        return "N/A"

def apa_citation(row):
    domain = urlparse(row["ì¶œì²˜"]).netloc.replace("www.", "")
    return f"{domain}. ({row['ì—°ë„']}). {row['ì œëª©']}. {row['ì¶œì²˜']}"

# =====================
# GPT Functions
# =====================
def generate_questions_and_keywords(topic, task_type):
    prompt = f"""
ì£¼ì œ: {topic}
ê³¼ì œ ìœ í˜•: {task_type}

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´.

{{
  "questions": [
    "ë¦¬ì„œì¹˜ ì§ˆë¬¸ 1",
    "ë¦¬ì„œì¹˜ ì§ˆë¬¸ 2",
    "ë¦¬ì„œì¹˜ ì§ˆë¬¸ 3"
  ],
  "keywords": [
    "í‚¤ì›Œë“œ1",
    "í‚¤ì›Œë“œ2",
    "í‚¤ì›Œë“œ3",
    "í‚¤ì›Œë“œ4",
    "í‚¤ì›Œë“œ5"
  ]
}}
"""
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return eval(res.choices[0].message.content)

def summarize_trends(keywords):
    prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜.
í‚¤ì›Œë“œ: {", ".join(keywords)}
"""
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return res.choices[0].message.content

# =====================
# News Search
# =====================
DEFAULT_BLOCK = ["ì—°ì˜ˆ", "ê°€ì‹­", "ìŠ¤ìº”ë“¤"]

def search_naver_news(keywords, user_blocks):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": naver_client_id,
        "X-Naver-Client-Secret": naver_client_secret
    }

    rows = []

    for kw in keywords:
        params = {"query": kw, "display": 10, "sort": "date"}
        res = requests.get(url, headers=headers, params=params)
        if res.status_code != 200:
            continue

        for item in res.json().get("items", []):
            title = clean_text(item["title"])
            desc = clean_text(item["description"])

            # ì‚¬ìš©ì ì œì™¸ í‚¤ì›Œë“œ + ê¸°ë³¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸
            if any(b in title for b in (DEFAULT_BLOCK + user_blocks)):
                continue

            # í‚¤ì›Œë“œ ìµœì†Œ 2ê°œ ì´ìƒ í¬í•¨
            match = sum(k in title + desc for k in keywords)
            if match < 2:
                continue

            date = parse_date(item["pubDate"])

            rows.append({
                "ì œëª©": title,
                "ìš”ì•½": desc,
                "ì‘ì„±ì¼": date,
                "ì—°ë„": date[:4],
                "ì¶œì²˜": item["originallink"],
                "ê´€ë ¨ë„": match
            })

    df = pd.DataFrame(rows).drop_duplicates()
    return df

# =====================
# UI
# =====================
st.title("ğŸ“š RefNote AI")
st.caption("ì¶œì²˜ ê¸°ë°˜ ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸")

topic = st.text_input("ì–´ë–¤ ì£¼ì œë¡œ ìë£Œë¥¼ ì¤€ë¹„í•˜ë‚˜ìš”?")
task_type = st.selectbox("ê³¼ì œ ìœ í˜•", ["ë°œí‘œ", "ë¦¬í¬íŠ¸", "ê¸°íšì„œ", "ë…¼ë¬¸"])

exclude_input = st.text_input(
    "ğŸš« ì œì™¸í•  í‚¤ì›Œë“œ (ì„ íƒ)",
    placeholder="ì˜ˆ: ì¸ì‚¬, ì›¨ë”©, ì‚¬ê±´"
)
user_blocks = [x.strip() for x in exclude_input.split(",") if x.strip()]

if st.button("ë¦¬ì„œì¹˜ ì‹œì‘") and topic:
    with st.spinner("ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘..."):
        parsed = generate_questions_and_keywords(topic, task_type)
        news_df = search_naver_news(parsed["keywords"], user_blocks)
        trend = summarize_trends(parsed["keywords"])

        st.session_state.result = {
            "questions": parsed["questions"],
            "keywords": parsed["keywords"],
            "trend": trend,
            "news": news_df
        }

# =====================
# Output
# =====================
if "result" in st.session_state:
    r = st.session_state.result

    st.subheader("ğŸ” ë¦¬ì„œì¹˜ ì§ˆë¬¸ (3ê°œ)")
    for q in r["questions"]:
        st.write("â€¢", q)

    st.subheader("ğŸ”‘ ê²€ìƒ‰ í‚¤ì›Œë“œ")
    st.write(", ".join(r["keywords"]))

    st.subheader("ğŸ§  ìµœì‹  ì—°êµ¬ ë™í–¥")
    st.write(r["trend"])

    st.subheader("ğŸ“Š ê·¼ê±° ìë£Œ")

    tab_news, tab_paper = st.tabs(["ğŸ“° ë‰´ìŠ¤", "ğŸ“„ ë…¼ë¬¸ (ì¤€ë¹„ ì¤‘)"])

    with tab_news:
        sort_option = st.radio(
            "ì •ë ¬ ê¸°ì¤€",
            ["ê´€ë ¨ë„ìˆœ", "ìµœì‹ ìˆœ"],
            horizontal=True
        )

        df = r["news"]
        if not df.empty:
            if sort_option == "ê´€ë ¨ë„ìˆœ":
                df = df.sort_values("ê´€ë ¨ë„", ascending=False)
            else:
                df = df.sort_values("ì‘ì„±ì¼", ascending=False)

            st.dataframe(df[["ì œëª©", "ì‘ì„±ì¼", "ì¶œì²˜"]], use_container_width=True)

            st.subheader("ğŸ“ ì°¸ê³ ë¬¸í—Œ (APA í˜•ì‹, TOP 10)")
            for i, row in enumerate(df.head(10).iterrows(), 1):
                st.write(f"{i}. {apa_citation(row[1])}")
        else:
            st.info("ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab_paper:
        st.info("DBpia ë“± ë…¼ë¬¸ API ì—°ë™ ì˜ˆì •ì…ë‹ˆë‹¤.")
