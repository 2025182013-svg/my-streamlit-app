import streamlit as st
import requests, html, json, os
from datetime import datetime
from openai import OpenAI
import pandas as pd
import io

# =====================
# ê¸°ë³¸ ì„¤ì •
# =====================
st.set_page_config(page_title="RefNote AI", layout="wide")
st.title("ğŸ“š RefNote AI")
st.caption("í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ ìƒì„± ë„êµ¬ (APA 7íŒ Â· CSV ë‹¤ìš´ë¡œë“œ Â· ê²€ìƒ‰ ë‚´ì—­ ì €ì¥)")

# =====================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =====================
if "results" not in st.session_state:
    st.session_state.results = None
if "history" not in st.session_state:
    st.session_state.history = []

HISTORY_FILE = "history.json"

# =====================
# ì‚¬ì´ë“œë°” - API
# =====================
st.sidebar.header("ğŸ”‘ API ì„¤ì •")
openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
naver_id = st.sidebar.text_input("Naver Client ID", type="password")
naver_secret = st.sidebar.text_input("Naver Client Secret", type="password")

if not openai_key or not naver_id or not naver_secret:
    st.warning("â¬…ï¸ ì‚¬ì´ë“œë°”ì— ëª¨ë“  API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=openai_key)

# =====================
# ìœ í‹¸
# =====================
def clean(t):
    return html.unescape(t).replace("<b>", "").replace("</b>", "").strip()

def parse_date(d):
    try:
        return datetime.strptime(d, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

def format_source(domain):
    return domain.replace("www.", "").split(".")[0].capitalize()

# APA 7íŒ ì›¹ ë‰´ìŠ¤ í˜•ì‹
def apa_news(row):
    author = row.get("ì €ì", row["ì¶œì²˜"])
    year = row["ë°œí–‰ì¼"][:4] if row["ë°œí–‰ì¼"] else "n.d."
    return f"{author}. ({year}). {row['ì œëª©']}. {row['ì¶œì²˜']}. {row['ë§í¬']}"

# =====================
# AI í•¨ìˆ˜
# =====================
def gen_questions(topic):
    prompt = f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ ì—°êµ¬ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”:\n{topic}"
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return [q.strip("-â€¢ ").strip() for q in r.choices[0].message.content.split("\n") if q.strip()]

def gen_keywords(topic):
    prompt = f"ë‹¤ìŒ ì£¼ì œì˜ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¤‘ìš”ë„ìˆœìœ¼ë¡œ ì‰¼í‘œë¡œ ì¶œë ¥:\n{topic}"
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return [k.strip() for k in r.choices[0].message.content.split(",")]

def gen_trend_summary(keywords):
    prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ì„¸ìš”.
í‚¤ì›Œë“œ: {', '.join(keywords)}
"""
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return r.choices[0].message.content.strip()

def relevance(topic, n):
    prompt = f"""
ì—°êµ¬ ì£¼ì œ: {topic}
ë‰´ìŠ¤ ì œëª©: {n['title']}
ìš”ì•½: {n['desc']}
ê´€ë ¨ë„ 0~3 ìˆ«ìë§Œ ì¶œë ¥
"""
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    try:
        return int(r.choices[0].message.content.strip())
    except:
        return 0

# =====================
# ë‰´ìŠ¤ ê²€ìƒ‰
# =====================
def search_news(q):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": naver_id,
        "X-Naver-Client-Secret": naver_secret
    }
    params = {"query": q, "display": 30, "sort": "date"}
    r = requests.get(url, headers=headers, params=params).json()

    out = []
    for i in r.get("items", []):
        out.append({
            "ì œëª©": clean(i["title"]),
            "ìš”ì•½": clean(i["description"]),
            "ì¶œì²˜": format_source(i["link"].split("/")[2]),
            "ë°œí–‰ì¼": parse_date(i["pubDate"]).strftime("%Y-%m-%d") if parse_date(i["pubDate"]) else "",
            "ë§í¬": i["link"]
        })
    return out

# =====================
# ë…¼ë¬¸(DBpia) ë¯¸êµ¬í˜„
# =====================
def search_dbpia(keyword):
    return pd.DataFrame(columns=["ì œëª©","ì €ì","í•™ìˆ ì§€","ì—°ë„","ë§í¬"])

# =====================
# ë¦¬ì„œì¹˜ ì‹¤í–‰
# =====================
topic = st.text_input("ì–´ë–¤ ì£¼ì œë¡œ ìë£Œë¥¼ ì¤€ë¹„í•˜ë‚˜ìš”?")

if st.button("ğŸ” ë¦¬ì„œì¹˜ ì‹œì‘") and topic:
    with st.spinner("ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘..."):
        questions = gen_questions(topic)
        keywords = gen_keywords(topic)

        # ë‰´ìŠ¤ ê²€ìƒ‰
        news_list = []
        for k in keywords[:2]:
            news_list.extend(search_news(k))

        # ê´€ë ¨ë„ í•„í„°ë§
        filtered = []
        for n in news_list:
            n["score"] = relevance(topic, n)
            if n["score"] >= 2:
                filtered.append(n)

        news_df = pd.DataFrame(filtered).drop_duplicates(subset=["ë§í¬"])
        paper_df = search_dbpia(topic)  # ë¯¸êµ¬í˜„

        trend_summary = gen_trend_summary(keywords)

        # ì„¸ì…˜ì— ì €ì¥
        st.session_state.results = {
            "topic": topic,
            "questions": questions,
            "keywords": keywords,
            "trend": trend_summary,
            "news": news_df,
            "papers": paper_df
        }

        # ê²€ìƒ‰ ë‚´ì—­ JSON ì €ì¥
        st.session_state.history.append(topic)
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(st.session_state.history, f, ensure_ascii=False, indent=2)

# =====================
# ê²°ê³¼ ì¶œë ¥
# =====================
if st.session_state.results:
    r = st.session_state.results

    st.subheader("ğŸ” ë¦¬ì„œì¹˜ ì§ˆë¬¸")
    for q in r["questions"]:
        st.markdown(f"â€¢ {q}")

    st.subheader("ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ")
    st.write(", ".join(r["keywords"]))

    st.subheader("ğŸ“ˆ ìµœì‹  ì—°êµ¬ ë™í–¥")
    st.markdown(r["trend"])

    # ë‰´ìŠ¤ / ë…¼ë¬¸ íƒ­
    tab_news, tab_paper = st.tabs(["ğŸ“° ë‰´ìŠ¤", "ğŸ“„ ë…¼ë¬¸ (DBpia ì˜ˆì •)"])

    # ---------------------
    # ë‰´ìŠ¤ íƒ­
    # ---------------------
    with tab_news:
        sort = st.radio("ì •ë ¬ ê¸°ì¤€", ["ê´€ë ¨ë„ìˆœ", "ìµœì‹ ìˆœ"], horizontal=True)
        news_table = r["news"]
        if sort == "ê´€ë ¨ë„ìˆœ":
            news_table = news_table.sort_values(by="score", ascending=False)
        else:
            news_table = news_table.sort_values(by="ë°œí–‰ì¼", ascending=False)

        st.dataframe(news_table, use_container_width=True)

        # CSV ë‹¤ìš´ë¡œë“œ
        csv_news = news_table.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ ë‰´ìŠ¤ ë¦¬ì„œì¹˜ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_news,
            file_name=f"{r['topic']}_news.csv",
            mime="text/csv"
        )

        # APA ìƒìœ„ 10ê°œ
        st.subheader("ğŸ“ ë‰´ìŠ¤ ì°¸ê³ ë¬¸í—Œ (APA 7íŒ Â· ìƒìœ„ 10ê°œ)")
        for _, row in news_table.head(10).iterrows():
            st.markdown(f"- {apa_news(row)}")

    # ---------------------
    # ë…¼ë¬¸ íƒ­
    # ---------------------
    with tab_paper:
        st.info("DBpia ì—°ë™ ì˜ˆì • ì˜ì—­ì…ë‹ˆë‹¤.")
        st.dataframe(r["papers"], use_container_width=True)

        # CSV ë‹¤ìš´ë¡œë“œ (ë¹ˆ ë°ì´í„°í”„ë ˆì„)
        csv_paper = r["papers"].to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ ë…¼ë¬¸ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_paper,
            file_name=f"{r['topic']}_papers.csv",
            mime="text/csv"
        )

# =====================
# ì‚¬ì´ë“œë°” - ê²€ìƒ‰ ë‚´ì—­ ë³µì›
# =====================
st.sidebar.header("ğŸ“‚ ë¦¬ì„œì¹˜ íˆìŠ¤í† ë¦¬")
# ì €ì¥ëœ JSON íŒŒì¼ ì½ê¸°
if os.path.exists(HISTORY_FILE):
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        saved_history = json.load(f)
else:
    saved_history = []

for h in reversed(saved_history):
    if st.sidebar.button(h):
        st.session_state.results = st.session_state.results or {}
        st.session_state.results["topic"] = h
        st.info(f"'{h}' ì£¼ì œ ì„ íƒë¨. ë¦¬ì„œì¹˜ ì¬ì‹¤í–‰ ê°€ëŠ¥.")
