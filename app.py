import streamlit as st
import requests, html
from datetime import datetime
from openai import OpenAI
import pandas as pd

# =====================
# ê¸°ë³¸ ì„¤ì •
# =====================
st.set_page_config(page_title="RefNote AI", layout="wide")
st.title("ğŸ“š RefNote AI")
st.caption("ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸ (ë‰´ìŠ¤ + í•™ìˆ  + ìµœì‹  ì—°êµ¬ ë™í–¥)")

# =====================
# ì„¸ì…˜ ìƒíƒœ
# =====================
if "results" not in st.session_state:
    st.session_state.results = None
if "history" not in st.session_state:
    st.session_state.history = []

# =====================
# ì‚¬ì´ë“œë°” - API
# =====================
st.sidebar.header("ğŸ”‘ API ì„¤ì •")
openai_key = st.sidebar.text_input("OpenAI API Key", type="password")
naver_id = st.sidebar.text_input("Naver Client ID", type="password")
naver_secret = st.sidebar.text_input("Naver Client Secret", type="password")
dbpia_key = st.sidebar.text_input("DBpia API Key", type="password")

if not openai_key or not naver_id or not naver_secret:
    st.warning("â¬…ï¸ API í‚¤ë“¤ì„ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=openai_key)

# =====================
# ìœ í‹¸
# =====================
def clean(t): return html.unescape(t).replace("<b>", "").replace("</b>", "").strip()
def parse_date(d):
    try:
        return datetime.strptime(d, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

def format_source(domain):
    domain = domain.replace("www.", "")
    base = domain.split(".")[0]
    return base.capitalize()

# =====================
# AI ì„œë¨¸ë¦¬
# =====================
def gen_trend_summary(keywords):
    prompt = f"""
    ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ìš”ì•½í•˜ì‹œì˜¤:
    {', '.join(keywords)}
    """
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"user","content":prompt}],
        temperature=0.2
    )
    return r.choices[0].message.content.strip()

# =====================
# ë„¤ì´ë²„ ë‰´ìŠ¤
# =====================
def search_news(q):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": naver_id,
        "X-Naver-Client-Secret": naver_secret
    }
    params = {"query": q, "display": 30, "sort": "date"}
    r = requests.get(url, headers=headers, params=params).json()
    out = []
    for i in r.get("items", []):
        out.append({
            "title": clean(i["title"]),
            "desc": clean(i["description"]),
            "link": i["link"],
            "date": parse_date(i["pubDate"])
        })
    return out

# =====================
# DBpia ê²€ìƒ‰ í•¨ìˆ˜ (ë‚˜ì¤‘ì— ì±„ì›Œ)
# =====================
def search_dbpia(q):
    # TODO: DBpia API ì—°ê²°
    # ì˜ˆ) requests.get("DBpiaURL?apikey=...")
    return []

# =====================
# ë¦¬ì„œì¹˜ ì‹¤í–‰
# =====================
topic = st.text_input("ì–´ë–¤ ì£¼ì œë¡œ ìë£Œë¥¼ ì¤€ë¹„í•˜ë‚˜ìš”?")
task_type = st.selectbox("ê³¼ì œ ìœ í˜•", ["ë…¼ë¬¸", "ë°œí‘œ"])
if st.button("ğŸ” ë¦¬ì„œì¹˜ ì‹œì‘") and topic:

    with st.spinner("ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘..."):
        # í‚¤ì›Œë“œ ìƒì„±
        kws = gen_keywords(topic)

        # ë‰´ìŠ¤
        news_raw = []
        for k in kws[:2]:
            news_raw.extend(search_news(k))

        # í•™ìˆ 
        dbpia_raw = []
        if dbpia_key:
            for k in kws[:3]:
                dbpia_raw.extend(search_dbpia(k))

        # ê´€ë ¨ë„ í‰ê°€ (ë‰´ìŠ¤ë§Œ)
        filtered_news = []
        for n in news_raw:
            s = relevance(topic, n)
            if s >= 2:
                n["score"] = s
                filtered_news.append(n)

        # ë‰´ìŠ¤ DataFrame
        news_df = pd.DataFrame([
            {
                "ì œëª©": n["title"],
                "ìš”ì•½": n["desc"],
                "ë„ë©”ì¸": n["link"].split("/")[2],
                "ì¶œì²˜": format_source(n["link"].split("/")[2]),
                "ì—°ë„": n["date"].year if n["date"] else "",
                "ê´€ë ¨ë„": n["score"],
                "ë§í¬": n["link"]
            } for n in filtered_news
        ]).drop_duplicates(subset=["ë§í¬"])

        # í•™ìˆ  DataFrame (ì•„ì§ êµ¬ì¡° ì˜ˆì‹œ)
        dbpia_df = pd.DataFrame(dbpia_raw)

        # ìµœì‹ ë™í–¥ ìš”ì•½
        trend_summary = gen_trend_summary(kws)

        # ê²°ê³¼ ì €ì¥
        st.session_state.results = {
            "topic": topic,
            "keywords": kws,
            "news": news_df,
            "dbpia": dbpia_df,
            "trend": trend_summary
        }
        st.session_state.history.append(topic)

# =====================
# ê²°ê³¼ ì¶œë ¥
# =====================
if st.session_state.results:
    r = st.session_state.results

    st.subheader("ğŸ” í‚¤ì›Œë“œ")
    st.write(", ".join(r["keywords"]))

    st.subheader("ğŸ“Œ ìµœì‹  ì—°êµ¬ ë™í–¥ ìš”ì•½")
    st.markdown(r["trend"])

    # ë‰´ìŠ¤ ì„¹ì…˜
    st.subheader("ğŸ“° ë‰´ìŠ¤ ê¸°ë°˜ ìë£Œ")
    st.dataframe(r["news"])

    # í•™ìˆ  ì„¹ì…˜
    st.subheader("ğŸ“„ í•™ìˆ  ìë£Œ (DBpia)")
    st.dataframe(r["dbpia"])

    # APA ì°¸ê³ ë¬¸í—Œ
    st.subheader("ğŸ“ ì°¸ê³ ë¬¸í—Œ (APA 7íŒ)")
    if not r["news"].empty:
        st.markdown("**ë‰´ìŠ¤**")
        for _, row in r["news"].iterrows():
            st.markdown(
                f"- {row['ì¶œì²˜']}. ({row['ì—°ë„']}). {row['ì œëª©']}. {row['ë§í¬']}"
            )
    if not r["dbpia"].empty:
        st.markdown("**í•™ìˆ ë…¼ë¬¸**")
        for _, row in r["dbpia"].iterrows():
            st.markdown(
                f"- {row['authors']} ({row['year']}). {row['title']}. {row['journal']}. {row['link']}"
            )

# =====================
# ì‚¬ì´ë“œë°” - íˆìŠ¤í† ë¦¬
# =====================
st.sidebar.header("ğŸ“‚ ë¦¬ì„œì¹˜ íˆìŠ¤í† ë¦¬")
for h in reversed(st.session_state.history):
    st.sidebar.write(f"â€¢ {h}")
